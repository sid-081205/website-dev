[
    {
      "objectID": "contributions.html",
      "href": "contributions.html",
      "title": "Contributions",
      "section": "",
      "text": "Adnan Baig | BSc in Economics\nAnna Jin | BSc in Economics\nHailey Stevens | BA in History"
    },
    {
      "objectID": "contributions.html#team-members",
      "href": "contributions.html#team-members",
      "title": "Contributions",
      "section": "",
      "text": "Adnan Baig | BSc in Economics\nAnna Jin | BSc in Economics\nHailey Stevens | BA in History"
    },
    {
      "objectID": "contributions.html#guidance",
      "href": "contributions.html#guidance",
      "title": "Contributions",
      "section": "Guidance",
      "text": "Guidance\n\nAlexander Soldatkin\nJon Cardoso-Silva"
    },
    {
      "objectID": "collection.html",
      "href": "collection.html",
      "title": "Data Collection",
      "section": "",
      "text": "The flowchart describes a step-by-step process for scraping data from the subreddit r/wallstreetbets. Here’s the process broken down into sequential steps:\n\nUsing Reddit API: The process begins by attempting to scrape data from the r/wallstreetbets subreddit using the Reddit API. The API is successfully accessed using a tool like Postman, but there’s a limitation encountered: it’s not possible to filter posts by date.\nAccessing Pushshift API: Since the Reddit API has limitations, the next step is to try using the Pushshift API, which contains post data from all subreddits, including historical data from Reddit. However, there’s an issue as the terms of conditions for the Pushshift API have changed and it is no longer available for research purposes, preventing access to this data.\nScraping Hidden Reddit API Data by Date: Despite not being able to use the Pushshift API for this purpose, the process finds success by scraping hidden Reddit API data with the ability to filter by date, thus overcoming the limitation faced in the first step.\nAttempted Scraping Using Selenium: There’s also an attempt made to scrape the subreddit using Selenium, a tool for automating web browsers. This attempt results in only being able to scrape the 1000 most recent posts from r/wallstreetbets, which suggests a limitation on the volume of data that can be scraped in a single go.\nUsing Another Person’s Pushshift API Access: There’s an attempt to use someone else’s access to the Pushshift API. Unfortunately, this does not work and access to the data through this method is unsuccessful.\nRequesting CSV Version of Reddit API: Finally, there’s a request made for a CSV version of the Reddit API data. This step is marked as a success, indicating that obtaining a CSV file with the desired data from the Reddit API was achieved.\n\nThroughout these steps, there are various attempts and strategies employed to overcome the limitations of different APIs and methods for scraping data from Reddit. The process shows persistence in finding a solution to extract the required data, eventually leading to a successful outcome.",
      "crumbs": [
        "Data Collection"
      ]
    },
    {
      "objectID": "collection.html#rwallstreetbets-posts",
      "href": "collection.html#rwallstreetbets-posts",
      "title": "Data Collection",
      "section": "",
      "text": "The flowchart describes a step-by-step process for scraping data from the subreddit r/wallstreetbets. Here’s the process broken down into sequential steps:\n\nUsing Reddit API: The process begins by attempting to scrape data from the r/wallstreetbets subreddit using the Reddit API. The API is successfully accessed using a tool like Postman, but there’s a limitation encountered: it’s not possible to filter posts by date.\nAccessing Pushshift API: Since the Reddit API has limitations, the next step is to try using the Pushshift API, which contains post data from all subreddits, including historical data from Reddit. However, there’s an issue as the terms of conditions for the Pushshift API have changed and it is no longer available for research purposes, preventing access to this data.\nScraping Hidden Reddit API Data by Date: Despite not being able to use the Pushshift API for this purpose, the process finds success by scraping hidden Reddit API data with the ability to filter by date, thus overcoming the limitation faced in the first step.\nAttempted Scraping Using Selenium: There’s also an attempt made to scrape the subreddit using Selenium, a tool for automating web browsers. This attempt results in only being able to scrape the 1000 most recent posts from r/wallstreetbets, which suggests a limitation on the volume of data that can be scraped in a single go.\nUsing Another Person’s Pushshift API Access: There’s an attempt to use someone else’s access to the Pushshift API. Unfortunately, this does not work and access to the data through this method is unsuccessful.\nRequesting CSV Version of Reddit API: Finally, there’s a request made for a CSV version of the Reddit API data. This step is marked as a success, indicating that obtaining a CSV file with the desired data from the Reddit API was achieved.\n\nThroughout these steps, there are various attempts and strategies employed to overcome the limitations of different APIs and methods for scraping data from Reddit. The process shows persistence in finding a solution to extract the required data, eventually leading to a successful outcome.",
      "crumbs": [
        "Data Collection"
      ]
    },
    {
      "objectID": "collection.html#gamestop-stock-prices",
      "href": "collection.html#gamestop-stock-prices",
      "title": "Data Collection",
      "section": "GameStop Stock Prices",
      "text": "GameStop Stock Prices\nThe image you’ve provided appears to be a diagram representing a process for obtaining stock data. Here’s a step-by-step explanation of the process depicted:\n\nConnect to the Alpha Vantage API: Alpha Vantage provides APIs for accessing stock market data. To start the process, you need to connect to the Alpha Vantage API, which typically involves using an API key that you can obtain from Alpha Vantage’s website.\nScrape all GameStop stock data from December 2020 to present day: Once connected to the API, the next step is to scrape, or programmatically request, the stock data for GameStop (GME) covering the period from December 2020 to the current date. You would use the appropriate API endpoints to request historical data for the specified period.\nSUCCESS: This indicates that the data retrieval was successful. In the context of a program or script, this could mean that the data was successfully fetched and possibly saved to a database or a file for further analysis.",
      "crumbs": [
        "Data Collection"
      ]
    },
    {
      "objectID": "data_overview.html",
      "href": "data_overview.html",
      "title": "Data Overview",
      "section": "",
      "text": "&lt;!DOCTYPE html&gt;\n\n\n\nInteractive Plot\n\n\n\n\nInteractive Plot\n\n\n\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\nInteractive Timeline\n\n\n\n\nInteractive Timeline",
      "crumbs": [
        "Data Overview"
      ]
    },
    {
      "objectID": "index.html",
      "href": "index.html",
      "title": "Dabbling in Data",
      "section": "",
      "text": "2020-21 GameStop Short Squeeze: an analysis of the profound impact of social meedia communities on the price of GameStop stock."
    },
    {
      "objectID": "index.html#project-overview",
      "href": "index.html#project-overview",
      "title": "Dabbling in Data",
      "section": "Project Overview",
      "text": "Project Overview\nWelcome to Dabbling in Data, a project which delves into the GameStop (GME) short squeeze phenomenon. Our primary objective is to shed light on the causation and degree of influence the Reddit community, r/wallstreetbets, had on the stock price of GameStop. We conducted an analysis of GME stock data in relation to r/wallstreetbets posts, over the period December 2020 to March 2021."
    },
    {
      "objectID": "index.html#our-motivation",
      "href": "index.html#our-motivation",
      "title": "Dabbling in Data",
      "section": "Our Motivation",
      "text": "Our Motivation\nOur project stems from a fascination with this event and its disruption to traditional financial norms, its empowerment of online communities, and its ensuing impact on regulation. Through our analysis, we aim to reveal the role of online communities on stock market behaviour."
    },
    {
      "objectID": "index.html#contextualising-our-project",
      "href": "index.html#contextualising-our-project",
      "title": "Dabbling in Data",
      "section": "Contextualising Our Project",
      "text": "Contextualising Our Project\nThe GameStop short squeeze in early 2021 saw a remarkable surge in the stock price of GameStop (GME). This surge was orchestrated by retail investors, particularly through the Reddit community r/wallstreetbets. As these investors bought GME shares en masse, it forced hedge funds and institutional investors, who had bet against the stock, to cover their positions, causing GME’s price to skyrocket.\n\n&lt;img src=\"../images/tree.png\" alt=\"Tree\"/&gt;"
    },
    {
      "objectID": "cleaning.html",
      "href": "cleaning.html",
      "title": "Data Cleaning",
      "section": "",
      "text": "The image you’ve uploaded describes a process for cleaning and preparing data from the subreddit r/wallstreetbets for analysis. Here’s a step-by-step explanation:\n\nr/wallstreetbets Data: This is the raw data from the r/wallstreetbets subreddit. It likely contains various fields of metadata for each post, such as scores, number of comments, and timestamps.\nFormatted ‘score’ and ‘num_comments’ columns to ‘int’: The first step in processing the data is to format the ‘score’ and ‘num_comments’ fields as integers. This is necessary because these fields are often numerical and need to be in a consistent format (integer type) for quantitative analysis.\nFormatted ‘date’ column to ‘datetime64’, consistent with GME stock price data: Next, the ‘date’ field is formatted to ‘datetime64’, a specific date-time format in many programming languages that allows for precise time series analysis. This step is crucial for aligning the Reddit data with GameStop (GME) stock price data, which will also be in a date-time format.\nUsed groupby method to group posts from same date: The third step is to group the posts by date. This is done using a ‘groupby’ method, which is a common operation in data manipulation libraries like pandas in Python. Grouping by date allows you to aggregate data, such as calculating the total number of posts, average score, or total number of comments for each date.\nOutcome: cleaned dataframe of Reddit data: The final output is a cleaned dataframe (a table-like data structure) containing the processed Reddit data. A cleaned dataframe typically means that the data is free of inconsistencies, is formatted correctly, and is ready for analysis or to be merged with other datasets, like the GME stock price data.\n\nThis process is typical of data preprocessing in data science, where raw data is transformed into a more usable format before analysis.",
      "crumbs": [
        "Data Cleaning"
      ]
    },
    {
      "objectID": "cleaning.html#rwallstreetbets-posts",
      "href": "cleaning.html#rwallstreetbets-posts",
      "title": "Data Cleaning",
      "section": "",
      "text": "The image you’ve uploaded describes a process for cleaning and preparing data from the subreddit r/wallstreetbets for analysis. Here’s a step-by-step explanation:\n\nr/wallstreetbets Data: This is the raw data from the r/wallstreetbets subreddit. It likely contains various fields of metadata for each post, such as scores, number of comments, and timestamps.\nFormatted ‘score’ and ‘num_comments’ columns to ‘int’: The first step in processing the data is to format the ‘score’ and ‘num_comments’ fields as integers. This is necessary because these fields are often numerical and need to be in a consistent format (integer type) for quantitative analysis.\nFormatted ‘date’ column to ‘datetime64’, consistent with GME stock price data: Next, the ‘date’ field is formatted to ‘datetime64’, a specific date-time format in many programming languages that allows for precise time series analysis. This step is crucial for aligning the Reddit data with GameStop (GME) stock price data, which will also be in a date-time format.\nUsed groupby method to group posts from same date: The third step is to group the posts by date. This is done using a ‘groupby’ method, which is a common operation in data manipulation libraries like pandas in Python. Grouping by date allows you to aggregate data, such as calculating the total number of posts, average score, or total number of comments for each date.\nOutcome: cleaned dataframe of Reddit data: The final output is a cleaned dataframe (a table-like data structure) containing the processed Reddit data. A cleaned dataframe typically means that the data is free of inconsistencies, is formatted correctly, and is ready for analysis or to be merged with other datasets, like the GME stock price data.\n\nThis process is typical of data preprocessing in data science, where raw data is transformed into a more usable format before analysis.",
      "crumbs": [
        "Data Cleaning"
      ]
    },
    {
      "objectID": "cleaning.html#gamestop-stock-prices",
      "href": "cleaning.html#gamestop-stock-prices",
      "title": "Data Cleaning",
      "section": "GameStop Stock Prices",
      "text": "GameStop Stock Prices\nThe image you’ve provided is a flowchart describing a process for obtaining and cleaning stock data for GameStop (GME) from the Alpha Vantage API. Here is the step-by-step explanation:\n\nAlpha Vantage API: This is the source of the stock data. Alpha Vantage provides an API from which you can request financial data, including stock prices.\nScraped all daily data from December 2020 to present day: The first step in the process is to send a request to the Alpha Vantage API to obtain all available daily stock data for GME, starting from December 2020 up to the current day.\nFilter by date, from December 2020 to end of March 2021: After obtaining the data, the next step is to filter it to include only the data within a specific time frame. In this case, the data is filtered to retain only the information from December 2020 to the end of March 2021.\nConverted data from ‘objects’ to the following:\n\nDate = ‘datetime64’: The date data is converted to ‘datetime64’ format, which is a standard format for handling dates and times in data analysis, allowing for easy sorting, filtering, and manipulation based on time.\nOpen = ‘float64’\nHigh = ‘float64’\nLow = ‘float64’\nClose = ‘float64’: The stock prices for the open, high, low, and close of each day are converted to ‘float64’, which is a data type suitable for representing floating-point numbers and allows for precise numerical analysis.\nVolume = ‘int32’: The trading volume data is converted to ‘int32’, which is a data type used for representing integer numbers and is sufficient for volume data, which represents the number of shares traded and does not require a decimal point.\n\nOutcome: cleaned dataframe of GME stock data: The final outcome is a cleaned dataframe of the GME stock data. A cleaned dataframe is one where the data has been processed to ensure that it is in a uniform and appropriate format for analysis. It implies that the data is now structured and ready for any subsequent analysis or visualization tasks.\n\nThis flowchart outlines a common data preprocessing task in financial data analysis, where raw data from an API is collected, filtered by a specific time frame, converted to suitable data types, and then cleaned for further analysis.",
      "crumbs": [
        "Data Cleaning"
      ]
    }
  ]